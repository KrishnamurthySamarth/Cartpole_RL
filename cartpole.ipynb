{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86031f3-a0da-419c-9de1-109b5d12f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9636a93b-dcb9-4127-84ac-6851b12cc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328799d3-d887-4bb1-932a-094e73ab08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"C:\\\\Users\\\\Mouna\\\\OneDrive\\\\Pictures\\\\Desktop\\\\logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70d6948-71db-4e3f-9163-ee027df01f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda:env])\n",
    "model = PPO('MlpPolicy', env, device='cuda', verbose=1, tensorboard_log=log_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508e55cb-6de1-4964-a39f-f71091332230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\Mouna\\OneDrive\\Pictures\\Desktop\\logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 125  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 116          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093657635 |\n",
      "|    clip_fraction        | 0.115        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | -0.0175      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.41         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 47.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010907391 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.0853      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008984376 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 142          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071800933 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.607       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0189      |\n",
      "|    value_loss           | 58.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007944673 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 55.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011522215 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 61.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066571636 |\n",
      "|    clip_fraction        | 0.0642       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.82         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00818     |\n",
      "|    value_loss           | 25.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004924296 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    value_loss           | 48.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418063 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21eea527ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c240240a-93d1-406b-9dd2-8574f9b03a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouna\\anaconda3\\envs\\reinforcement\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mouna\\anaconda3\\envs\\reinforcement\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes=10,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35199c15-5a10-4ab9-ba04-119463dd16e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:0, score:[500.]\n",
      "ep:1, score:[500.]\n",
      "ep:2, score:[500.]\n",
      "ep:3, score:[500.]\n",
      "ep:4, score:[500.]\n"
     ]
    }
   ],
   "source": [
    "eps = 5\n",
    "for ep in range (eps) :\n",
    "    obs = env.reset()\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated,_ = env.step(action)\n",
    "        score +=reward\n",
    "    print(f'ep:{ep}, score:{score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44460182-78ba-4b1c-a32c-ec27678221e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9e42020-7ce4-4bfe-bf8d-6e0ab79d0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
    "evall_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88ce137f-1c22-4d6f-8d4d-a6ffe885cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model2 = PPO('MlpPolicy', env, device='cuda', verbose=1, tensorboard_log=log_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a48cae8c-47e1-4a6d-9be3-e6b0add878b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\Mouna\\OneDrive\\Pictures\\Desktop\\logs\\PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 240  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002531718 |\n",
      "|    clip_fraction        | 0.0128      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    value_loss           | 49.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007814667 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | -0.594      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.779       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000784   |\n",
      "|    value_loss           | 5.62        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040631397 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.544       |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    value_loss           | 32.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouna\\anaconda3\\envs\\reinforcement\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050321473 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.24         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21eea527ad0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=evall_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029cbd90-05ae-427c-9064-021b808c5c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
